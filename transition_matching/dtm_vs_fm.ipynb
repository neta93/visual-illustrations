{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Difference Transition Matching (DTM) vs. Flow Matching (FM) Demo\n",
    "\n",
    "This notebook is a small demo that shows the difference between DTM and FM in a 2 deminsional example, and reproduce my DTM vs. FM GIF.\n",
    "\n",
    "\n",
    "![Simulation Result](dtm_vs_fm.gif)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rb5VSo4mNkVd"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "from typing import Optional, List\n",
    "import io\n",
    "\n",
    "from tqdm import tqdm\n",
    "from IPython.display import display, Video, Image\n",
    "\n",
    "import torch\n",
    "from torch import nn, Tensor\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "import matplotlib.colors as mcolors\n",
    "import imageio.v2 as imageio\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = 'cuda:0'\n",
    "    print('Using gpu')\n",
    "else:\n",
    "    device = 'cpu'\n",
    "    print('Using cpu.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Activation class\n",
    "class Swish(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor: \n",
    "        return torch.sigmoid(x) * x\n",
    "\n",
    "# Model class\n",
    "class MLP(nn.Module):\n",
    "    input_dim:int = 3\n",
    "    output_dim:int = 2\n",
    "    def __init__(self, hidden_dim: int = 128, is_tm: bool = False):\n",
    "        super().__init__()\n",
    "        self.is_tm = is_tm\n",
    "        self.input_dim += 3*int(is_tm)\n",
    "\n",
    "        self.main = nn.Sequential(\n",
    "            nn.Linear(self.input_dim, hidden_dim),\n",
    "            Swish(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            Swish(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            Swish(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            Swish(),\n",
    "            nn.Linear(hidden_dim, self.output_dim),\n",
    "        )\n",
    "    \n",
    "    def forward(\n",
    "        self, \n",
    "        x: Tensor, \n",
    "        t: Tensor, \n",
    "        y: Optional[Tensor] = None, \n",
    "        s: Optional[Tensor] = None\n",
    "    ) -> Tensor:\n",
    "        x = x.reshape(-1, 2)\n",
    "        t = t.reshape(-1, 1)\n",
    "\n",
    "        if self.is_tm:\n",
    "            assert s is not None, \"s timesteps must be provided for transition matching\"\n",
    "            assert y is not None, \"y target parametrization must be provided for transition matching\"\n",
    "            s = s.reshape(-1, 1)\n",
    "            y = y.reshape(-1, 2)\n",
    "            h = torch.cat([x, t, y, s], dim=1)\n",
    "        else:\n",
    "            h = torch.cat([x, t], dim=1)\n",
    "        \n",
    "        output = self.main(h)\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gaussian Mixture Model (GMM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GaussianMixtureModel(torch.nn.Module):\n",
    "    def __init__(self, weights:Tensor, means:Tensor, sigmas:Tensor):\n",
    "        super().__init__()\n",
    "        self.weights = torch.nn.Parameter(\n",
    "            weights.float(), requires_grad=False\n",
    "        )\n",
    "        self.means = torch.nn.Parameter(\n",
    "            means.float(), requires_grad=False\n",
    "        )\n",
    "        self.sigmas = torch.nn.Parameter(\n",
    "            sigmas.float(), requires_grad=False\n",
    "        )\n",
    "    \n",
    "    def log_prob(self, x: Tensor):\n",
    "        means = self.means[None] \n",
    "        sigmas = self.sigmas[None]\n",
    "        weights = self.weights[None]\n",
    "        x = x[:,None]\n",
    "        # log prob of a single Guassian \n",
    "        gaussian_log_probs = -torch.sum((x-means)**2, dim=-1)/(2*sigmas**2) \\\n",
    "            - torch.log((2*torch.pi)**0.5*sigmas)\n",
    "        # log prob of a mixture of Gaussians weighted by 'weights' \n",
    "        mixture_log_probs = torch.logsumexp(torch.log(weights) + gaussian_log_probs, dim=-1)\n",
    "        \n",
    "        return mixture_log_probs\n",
    "        \n",
    "    def sample(self, num_samples: int):\n",
    "        # sample a single Gaussian index according to weights\n",
    "        index = torch.multinomial(self.weights, num_samples, replacement=True)\n",
    "        # sample from the selected Gaussians\n",
    "        normal = torch.randn(\n",
    "            size=(num_samples, self.means.size(-1)), device=self.means.device\n",
    "        )\n",
    "        sample = self.means[index] + self.sigmas[index][...,None] * normal\n",
    "        \n",
    "        return sample\n",
    "    \n",
    "    def forward(self, x: Tensor):\n",
    "        return torch.exp(self.log_prob(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Instantiate source and target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Init target distributions\n",
    "target = GaussianMixtureModel(\n",
    "    weights=torch.tensor(\n",
    "        [\n",
    "            0.66, # first component weight\n",
    "            0.34, # second component weight\n",
    "        ]\n",
    "    ),\n",
    "    means=torch.tensor(\n",
    "        [\n",
    "            [7, -1], # first component mean \n",
    "            [6, 1],  # second component mean\n",
    "        ]\n",
    "    ),\n",
    "    sigmas=torch.tensor(\n",
    "        [\n",
    "            0.80, # first component std \n",
    "            0.80, # second component std\n",
    "        ]\n",
    "    ),\n",
    ").to(device)\n",
    "# Init source distributions\n",
    "source = GaussianMixtureModel( # simple Normal distribution\n",
    "    weights=torch.tensor([1.0]),\n",
    "    means=torch.tensor([[0, 0]]),\n",
    "    sigmas=torch.tensor([1.0]),\n",
    ").to(device)\n",
    "\n",
    "def set_background(ax: plt.Axes):\n",
    "    \"\"\"Set axis background as source and target density.\"\"\"\n",
    "    limits = { # (x_min, x_max, y_min, y_max)\n",
    "        'source': (-2.5, 3, -4, 4),\n",
    "        'target': (3, 9.0, -4, 4)\n",
    "    }\n",
    "\n",
    "    density = {\n",
    "        'source': source,\n",
    "        'target': target\n",
    "    }\n",
    "\n",
    "    xx, yy, zz = {}, {}, {}\n",
    "\n",
    "    # grid size n_points x n_points\n",
    "    n_points = 200\n",
    "    for key in ['source', 'target']:\n",
    "        x_min, x_max, y_min, y_max = limits[key]\n",
    "        x = torch.linspace(x_min, x_max, n_points)\n",
    "        y = torch.linspace(y_min, y_max, n_points)\n",
    "        x_mesh, y_mesh = torch.meshgrid(x, y, indexing=\"ij\")\n",
    "        # store meshgrid for x and y\n",
    "        xx[key] = x_mesh.numpy()\n",
    "        yy[key] = y_mesh.numpy()\n",
    "        # Flat grid of shape [n_points**2, 2]\n",
    "        grid_points = torch.stack([x_mesh.flatten(), y_mesh.flatten()], dim=1).to(device)\n",
    "        # evaluate densities \n",
    "        z_mesh = density[key](grid_points).reshape(n_points, n_points).cpu()\n",
    "        # store meshgrid for z\n",
    "        zz[key] = z_mesh.numpy()\n",
    "\n",
    "    # custom colormap for the target distribution that combines low values from OrRd, high values from PuBu\n",
    "    # Sample points along OrRd (0 to ~0.5), and PuBu (0.5 to 1.0).\n",
    "    cutoff = 0.055\n",
    "    OrBu = mcolors.ListedColormap(\n",
    "        np.vstack(\n",
    "            [\n",
    "                cm.OrRd(np.linspace(0, cutoff, int((256*cutoff))))     , \n",
    "                cm.PuBu(np.linspace(cutoff, 1.0, int((256*(1-cutoff)))))\n",
    "            ]\n",
    "        ), name='OrBu'\n",
    "    )\n",
    "    cmaps = {\n",
    "        'source': cm.OrRd,\n",
    "        'target': OrBu,\n",
    "    }\n",
    "\n",
    "    # min and max values of densities to normalize color maps\n",
    "    vmin = np.stack(list(zz.values())).min()\n",
    "    vmax = np.stack(list(zz.values())).max()\n",
    "\n",
    "    for key in ['source', 'target']:\n",
    "        ax.contourf(\n",
    "            xx[key], \n",
    "            yy[key], \n",
    "            zz[key], \n",
    "            levels=10, \n",
    "            cmap=cmaps[key], \n",
    "            alpha=0.8,\n",
    "            vmin=vmin, \n",
    "            vmax=vmax,\n",
    "            zorder=0\n",
    "        )\n",
    "\n",
    "    ax.set_xlim(-2.5, 9.0)\n",
    "    ax.set_ylim(-4, 4)\n",
    "    ax.set_aspect('equal')\n",
    "    ax.axis('off')\n",
    "\n",
    "\n",
    "# Preview source and target distributions\n",
    "fig, ax = plt.subplots(1,1,figsize=(8,6))\n",
    "# Set background \n",
    "set_background(ax)\n",
    "\n",
    "fig.tight_layout()\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train DTM and FM models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, loss_fn):\n",
    "    # training arguments\n",
    "    lr = 0.001\n",
    "    batch_size = 4096\n",
    "    iterations = 50000\n",
    "    print_every = 10000 \n",
    "\n",
    "    # init optimizer\n",
    "    optim = torch.optim.Adam(model.parameters(), lr=lr) \n",
    "\n",
    "    # cosine annealing scheduler\n",
    "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optim, T_max=iterations, eta_min=0.00001)\n",
    "        \n",
    "    # train\n",
    "    start_time = time.time()\n",
    "    for i in range(iterations):\n",
    "        optim.zero_grad() \n",
    "        \n",
    "        # sample time \n",
    "        t = torch.rand(batch_size, 1, device=device)\n",
    "        # sample datapoint X_1\n",
    "        x_1 = target.sample(batch_size) \n",
    "        # Sample source X_0\n",
    "        x_0 = source.sample(batch_size)\n",
    "        # get X_t\n",
    "        x_t = (1-t)*x_0 + t*x_1\n",
    "        # get Y\n",
    "        y = x_1 - x_0\n",
    "\n",
    "        # compute loss\n",
    "        loss = loss_fn(model, x_t, t, y)\n",
    "\n",
    "        # optimizer step\n",
    "        loss.backward() \n",
    "        optim.step() \n",
    "        scheduler.step()\n",
    "        \n",
    "        # log loss\n",
    "        if (i+1) % print_every == 0:\n",
    "            elapsed = time.time() - start_time\n",
    "            print('| iter {:6d} | {:5.2f} ms/step | loss {:8.3f} ' \n",
    "                .format(i+1, elapsed*1000/print_every, loss.item())) \n",
    "            start_time = time.time()\n",
    "\n",
    "\n",
    "# Difference transition matching (DTM) and flow matching (FM)\n",
    "# share the same supervising process prameterization.\n",
    "# Linear process: (1-t)*X_0 + t*X_1.\n",
    "# Difference prediction: Y = X_1 - X_0.\n",
    "# However, they are different in thier choice of modeling, i.e., loss functions and architecture.\n",
    "\n",
    "def transition_matching_loss(model, x_t, t, y):\n",
    "    \"\"\"Apply flow matching loss with Y as target and Gaussian source.\"\"\"\n",
    "    # sample time \n",
    "    s = torch.rand_like(t)\n",
    "    # sample source Y_0\n",
    "    y_0 = torch.randn_like(y)\n",
    "    # get Y_s\n",
    "    y_s = (1-s)*y_0 + s*y\n",
    "    # Transition matching loss\n",
    "    loss = torch.nn.functional.mse_loss(\n",
    "        model(x_t, t, y_s, s), y - y_0\n",
    "    )\n",
    "    return loss\n",
    "\n",
    "def flow_matching_loss(model, x_t, t, y):\n",
    "    # Flow matching loss\n",
    "    loss = torch.nn.functional.mse_loss(\n",
    "        model(x_t, t), y\n",
    "    )\n",
    "    return loss\n",
    "\n",
    "# init model for dtm\n",
    "model_dtm = MLP(hidden_dim=512, is_tm=True).to(device) \n",
    "\n",
    "# init model for fm\n",
    "model_fm = MLP(hidden_dim=512).to(device)\n",
    "\n",
    "print(\"Training DTM model:\")\n",
    "train(model_dtm, transition_matching_loss)\n",
    "print(\"\\n\")\n",
    "print(\"Training FM model:\")\n",
    "train(model_fm, flow_matching_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sample DTM and FM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function that sample Y_t ~ p_{Y|X_t} using DTM model\n",
    "INNER_STEPS=32\n",
    "def dtm_fn(x_t,t, ):\n",
    "    y_s = torch.randn(x_t.size(0), 2).to(device)\n",
    "    s_grid = torch.linspace(0, 1, INNER_STEPS+1, device=device)\n",
    "    for s, ds in zip(s_grid[:-1], torch.diff(s_grid)):\n",
    "        u_s = model_dtm(x_t, t, y_s, s)\n",
    "        y_s = y_s + ds * u_s\n",
    "    \n",
    "    return y_s\n",
    "\n",
    "# function that sample Y_t=E[Y|X_t] using FM model\n",
    "def fm_fn(x_t, t):\n",
    "    return model_fm(x_t, t)\n",
    "\n",
    "@torch.no_grad()\n",
    "def sampler(model_fn, x_0, T):\n",
    "    # store t along the trajectory\n",
    "    traj_t = torch.empty(T+1,1, device=device)\n",
    "    # store X_t along the trajectory\n",
    "    traj_x = torch.empty(T+1,2, device=device)\n",
    "    # store Y_t along the trajectory\n",
    "    traj_y = torch.empty(T,2, device=device)\n",
    "\n",
    "    t = torch.zeros(1,1, device=device)\n",
    "    traj_t[0] = t\n",
    "    x_t = x_0.view(1, 2)\n",
    "    traj_x[0] = x_t\n",
    "    for i in range(T):\n",
    "        y_t = model_fn(x_t, t)\n",
    "        x_t = x_t + 1/T * y_t\n",
    "        t = t + 1/T\n",
    "\n",
    "        traj_t[i+1] = t\n",
    "        traj_x[i+1] = x_t\n",
    "        traj_y[i] = y_t\n",
    "    \n",
    "    return traj_t, traj_x, traj_y\n",
    "\n",
    "# number transition steps to plot T=2,4,8,32,128\n",
    "T_dtm = 2**torch.tensor([1, 2, 3, 5, 7], device=device)\n",
    "\n",
    "# seed for source points\n",
    "torch.manual_seed(2025)\n",
    "X_0 = torch.randn(len(T_dtm), 2).to(device)\n",
    "X_0[:-2] = torch.randn(3, 2).to(device)\n",
    "\n",
    "t_dtm = []\n",
    "X_dtm = []\n",
    "Y_dtm = []\n",
    "# seed for sampling the transition probability\n",
    "torch.manual_seed(111)\n",
    "for T, x_0 in zip(T_dtm, X_0):\n",
    "    traj_t, traj_x, traj_y = sampler(dtm_fn, x_0, T)\n",
    "    t_dtm.append(traj_t.detach().cpu())\n",
    "    X_dtm.append(traj_x.detach().cpu())\n",
    "    Y_dtm.append(traj_y.detach().cpu())\n",
    "\n",
    "# number transition steps for FM is always 100\n",
    "T_fm = torch.tensor([100]*len(T_dtm), device=device)\n",
    "t_fm = []\n",
    "X_fm = []\n",
    "Y_fm = []\n",
    "# FM transitions are deterministic so no need to set seed for reproducibility\n",
    "for T, x_0 in zip(T_fm, X_0):\n",
    "    traj_t, traj_x, traj_y = sampler(fm_fn, x_0, T)\n",
    "    t_fm.append(traj_t.detach().cpu())\n",
    "    X_fm.append(traj_x.detach().cpu())\n",
    "    Y_fm.append(traj_y.detach().cpu())\n",
    "\n",
    "is_preprocessed_for_plotting = False\n",
    "\n",
    "# preview of trajectories\n",
    "for i, T in enumerate(T_dtm):\n",
    "    fig, ax = plt.subplots(1,1,figsize=(8,6))\n",
    "    set_background(ax)\n",
    "    # DTM trajectory\n",
    "    ax.plot(\n",
    "        X_dtm[i][:, 0], \n",
    "        X_dtm[i][:, 1], \n",
    "        color='black', \n",
    "        linewidth=4,\n",
    "        label=f'DTM: {T.item():1d}-steps'\n",
    "    )\n",
    "    # FM trajectory\n",
    "    ax.plot(\n",
    "        X_fm[i][:, 0], \n",
    "        X_fm[i][:, 1], \n",
    "        color='black', \n",
    "        alpha=0.3,\n",
    "        linewidth=4,\n",
    "        label='FM'\n",
    "    )\n",
    "    \n",
    "    ax.legend(\n",
    "        fontsize=20,\n",
    "        loc='upper right',\n",
    "        bbox_to_anchor=(0.98, 0.98),  \n",
    "        borderaxespad=0.2,            \n",
    "        frameon=False                 \n",
    "    )\n",
    "    \n",
    "    ax.set_aspect('equal')\n",
    "    ax.axis('off')\n",
    "    fig.tight_layout()\n",
    "    fig.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make frames and GIF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not is_preprocessed_for_plotting:\n",
    "    # Downsample the FM trajectories for visualization\n",
    "    t_fm = [t[::4] for t in t_fm]\n",
    "    X_fm = [x_t[::4] for x_t in X_fm]\n",
    "    Y_fm = [y_t[::4] for y_t in Y_fm]\n",
    "\n",
    "    # The difference prediction Y = X_1 - X_0.\n",
    "    # Hence Y_t (i.e., Y | X_t) is plotted as \n",
    "    # the difference between data prediction X_1\n",
    "    # and noise prediction X_0. Noise prediction is \n",
    "    # computed with X_0 = X_t - t*Y_t and data prediction \n",
    "    # with X_1 = X_t + (1-t)*Y_t\n",
    "    Y_dtm = [ \n",
    "        torch.stack(\n",
    "            [x_t[:-1] - t[:-1]*y_t, x_t[:-1] + (1-t[:-1])*y_t],\n",
    "        dim=1\n",
    "        ) for x_t, y_t, t in zip(X_dtm, Y_dtm, t_dtm)\n",
    "\n",
    "    ]\n",
    "\n",
    "    Y_fm = [ \n",
    "        torch.stack(\n",
    "            [x_t[:-1] - t[:-1]*y_t, x_t[:-1] + (1-t[:-1])*y_t],\n",
    "        dim=1\n",
    "        ) for x_t, y_t, t in zip(X_fm, Y_fm, t_fm)\n",
    "\n",
    "    ]\n",
    "    is_preprocessed_for_plotting = True\n",
    "\n",
    "# set colors\n",
    "blue = '#4C72B0'\n",
    "red = '#C44E52'\n",
    "green = \"#81C784\"\n",
    "yellow = \"#FFF176\"\n",
    "\n",
    "def set_legend(ax: plt.Axes, source_point: Tensor, T_dtm: int):\n",
    "    x, y = source_point.view(-1)\n",
    "    # plot a single point line of DTM for the legend\n",
    "    ax.plot(\n",
    "        x, \n",
    "        y,\n",
    "        color='black', \n",
    "        linewidth=4,\n",
    "        label=f'DTM: {T_dtm:1d}-steps'\n",
    "    )\n",
    "\n",
    "    # plot a single point line of FM for the legend\n",
    "    ax.plot(\n",
    "        x, \n",
    "        y,\n",
    "        color= \"#9E9E9E\",\n",
    "        alpha=0.8,\n",
    "        linewidth=4,\n",
    "        label='FM'\n",
    "    )\n",
    "\n",
    "    ax.legend(\n",
    "        fontsize=20,\n",
    "        loc='upper right',\n",
    "        bbox_to_anchor=(0.98, 0.98),  \n",
    "        borderaxespad=0.2,            \n",
    "        frameon=False                 \n",
    "    )\n",
    "\n",
    "# Flag that set whether to save frames to disk or only save the GIF.\n",
    "SAVE_TO_DISK = False\n",
    "# set DPI to 150 or 300 for full size.\n",
    "DPI = 150\n",
    "frames = []\n",
    "def save_frame(fig: plt.Figure, name_counter: int):\n",
    "    # set target\n",
    "    if SAVE_TO_DISK:\n",
    "        os.makedirs('dtm_vs_fm', exist_ok=True)\n",
    "        target = f'dtm_vs_fm/{name_counter:03d}.png'\n",
    "    else:\n",
    "        target = io.BytesIO()\n",
    "    # save frame to target\n",
    "    fig.savefig(target, format='png', dpi=DPI, bbox_inches='tight')\n",
    "    # read the frame back\n",
    "    if SAVE_TO_DISK:\n",
    "        frames.append(imageio.imread(target))\n",
    "    else:\n",
    "        target.seek(0)\n",
    "        frames.append(imageio.imread(target))\n",
    "        target.close() \n",
    "\n",
    "name_counter = 0\n",
    "for i, T in enumerate(T_dtm):\n",
    "    print(f'Generating frames for T={T.item():1d}-steps DTM vs FM:')\n",
    "    # save first frame\n",
    "    fig, ax = plt.subplots(1,1,figsize=(8,6))\n",
    "    set_background(ax)\n",
    "    ax.set_xlim(-2.5, 9.0)\n",
    "    ax.set_ylim(-4, 4)\n",
    "    ax.set_aspect('equal')\n",
    "    ax.axis('off')\n",
    "    set_legend(\n",
    "        ax, \n",
    "        source_point=X_fm[i][0], \n",
    "        T_dtm=T.item()\n",
    "    )\n",
    "    # scatter source point in red\n",
    "    ax.scatter(\n",
    "        X_fm[i][0, 0], X_fm[i][0, 1], s=128, c=red, alpha=1.0, zorder=3\n",
    "    )\n",
    "    # save frame\n",
    "    fig.tight_layout()\n",
    "    save_frame(fig, name_counter)\n",
    "    name_counter += 1\n",
    "\n",
    "    # plot frames of FM trajectory \n",
    "    for j in range(len(X_fm[i])):\n",
    "        fig, ax = plt.subplots(1,1,figsize=(8,6))\n",
    "        set_background(ax)\n",
    "        set_legend(\n",
    "            ax, \n",
    "            source_point=X_fm[i][0], \n",
    "            T_dtm=T.item()\n",
    "        )\n",
    "        # plot FM trajectory up to step j\n",
    "        ax.plot(\n",
    "            X_fm[i][:j+1, 0], \n",
    "            X_fm[i][:j+1, 1], \n",
    "            color= \"#9E9E9E\",\n",
    "            alpha=0.8,\n",
    "            linewidth=4,\n",
    "        )\n",
    "        # scatter source point in red\n",
    "        ax.scatter(\n",
    "            X_fm[i][0, 0], X_fm[i][0, 1], s=128, c=red, alpha=1.0, zorder=3\n",
    "        )\n",
    "        # if j is the last step, scatter the end of the line in blue\n",
    "        if j == len(X_fm[i])-1:\n",
    "            ax.scatter(\n",
    "                X_fm[i][j, 0], X_fm[i][j, 1], s=128, c=blue, alpha=1.0, zorder=3\n",
    "            )\n",
    "        # else, scatter the end of the line in gray,\n",
    "        # and plot the j-th Y_t.\n",
    "        else:\n",
    "            ax.scatter(\n",
    "                X_fm[i][j, 0], X_fm[i][j, 1], s=128, \n",
    "                color= \"#9E9E9E\", alpha=0.8,\n",
    "                zorder=3\n",
    "            )\n",
    "            ax.plot(\n",
    "                    Y_fm[i][j,:, 0],\n",
    "                    Y_fm[i][j,:, 1], \n",
    "                    color=yellow,\n",
    "                    linewidth=4,\n",
    "                    marker='o', \n",
    "                    markersize=11.31,\n",
    "                    zorder=1,\n",
    "                )\n",
    "        \n",
    "        # save frame\n",
    "        fig.tight_layout()\n",
    "        save_frame(fig, name_counter)\n",
    "        name_counter += 1\n",
    "        plt.close('all')\n",
    "    \n",
    "    \n",
    "    # plot frames of DTM trajectory \n",
    "    for j in tqdm(range(len(X_dtm[i]))):\n",
    "        # For DTM Y_t is dissolved into the plot using the alpha scale.\n",
    "        # To prevent the GIF from being too long, for larger number of steps T,\n",
    "        # only a subset od the alpha scales are used.\n",
    "        alpha_scale = [0, 0.25, 0.5, 0.75, 1.0, 1.0]\n",
    "\n",
    "        if T in [2, 4]:\n",
    "            k_indices = [0, 1, 2, 3, 4, 5]\n",
    "        \n",
    "        elif T == 8:\n",
    "            k_indices = [0, 2, 4, 5]\n",
    "        \n",
    "        elif T == 32:\n",
    "            k_indices = [0, 4, 5]\n",
    "        \n",
    "        elif T == 128:\n",
    "            k_indices = [5]\n",
    "        \n",
    "        def make_step(k):\n",
    "            return k == k_indices[-1]\n",
    "        \n",
    "        for k in k_indices:\n",
    "            fig, ax = plt.subplots(1,1,figsize=(8,6))\n",
    "            set_background(ax)\n",
    "            set_legend(\n",
    "                ax, \n",
    "                source_point=X_fm[i][0], \n",
    "                T_dtm=T.item()\n",
    "            )\n",
    "            # scatter source point in red\n",
    "            ax.scatter(\n",
    "                X_fm[i][0, 0], X_fm[i][0, 1], s=128, c=red, alpha=1.0, zorder=3\n",
    "            )\n",
    "            # plot the full FM trajectory\n",
    "            ax.plot(\n",
    "                X_fm[i][:, 0], \n",
    "                X_fm[i][:, 1], \n",
    "                color= \"#9E9E9E\",\n",
    "                alpha=0.8,\n",
    "                linewidth=4,\n",
    "            )\n",
    "            # scatter the end point of FM trajectory in blue\n",
    "            ax.scatter(\n",
    "                    X_fm[i][-1, 0], X_fm[i][-1, 1], s=128, c=blue, alpha=1.0, zorder=3\n",
    "                )\n",
    "            \n",
    "            alpha = alpha_scale[k]\n",
    "            if j == len(X_dtm[i])-1:\n",
    "                # scatter the end point of DTM trajectory in blue\n",
    "                ax.scatter(\n",
    "                    X_dtm[i][-1, 0], X_dtm[i][-1, 1], s=128, c=blue, alpha=1.0, zorder=3\n",
    "                )\n",
    "            else:\n",
    "                # else, scatter the end of the line in black,\n",
    "                # and plot the j-th Y_t.\n",
    "                ax.plot(\n",
    "                        Y_dtm[i][j,:, 0],  \n",
    "                        Y_dtm[i][j,:, 1],  \n",
    "                        color=green,\n",
    "                        alpha=alpha,\n",
    "                        linewidth=4,\n",
    "                        marker='o', \n",
    "                        markersize=11.31,\n",
    "                    )\n",
    "                ax.scatter(\n",
    "                    X_dtm[i][j+int(make_step(k)), 0], \n",
    "                    X_dtm[i][j+int(make_step(k)), 1], \n",
    "                    s=128, \n",
    "                    c='black',\n",
    "                    alpha=1.0, \n",
    "                    zorder=3,\n",
    "                )\n",
    "            # plot FM trajectory up to step j + int(make_step(k))\n",
    "            ax.plot(\n",
    "                X_dtm[i][:j+1+int(make_step(k)), 0], \n",
    "                X_dtm[i][:j+1+int(make_step(k)), 1], \n",
    "                color='black', \n",
    "                linewidth=4,\n",
    "            )\n",
    "            # save frame\n",
    "            fig.tight_layout()\n",
    "            save_frame(fig, name_counter)\n",
    "            name_counter += 1\n",
    "            plt.close('all')\n",
    "\n",
    "\n",
    "GIF = True\n",
    "# Create GIF or MP4 video from saved frames\n",
    "if GIF:\n",
    "    # Save as GIF\n",
    "    imageio.mimsave(\"dtm_vs_fm.gif\", frames, duration=1/24)\n",
    "    display(Image(\"dtm_vs_fm.gif\"))\n",
    "else:\n",
    "    # Save as MP4 video\n",
    "    imageio.mimwrite(\"dtm_vs_fm.mp4\", frames, fps=24)\n",
    "    display(Video(\"dtm_vs_fm.mp4\", width=500, height=300))\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "g8QtNgs1-PlE",
    "wW3VMmrK2t2d",
    "_7aH8D0H3IJT"
   ],
   "name": "scalable_CNF.ipynb",
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "test0001",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
